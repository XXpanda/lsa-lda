#labels Featured
= Content =
<wiki:toc max_depth="2" />

= Download =
Download Code: [http://lsa-lda.googlecode.com/files/ldaviagibbs.zip Code]<br>
Download sample model: [http://lsa-lda.googlecode.com/files/model.zip Sample]
= Compile =
Source code contains two program: lda and topword. lda program do parameter estimation & inference, and topword program outputs the top words of each topic.<br>
1. Compile lda: <br>
   $> make clean<br>
   $> make<br>
2. Compile topword:<br>
   $> make topword<br>
= Parameter estimation =
Command : $> ./lda |topic num| |sample num| |data| |model name|<br>
 * topic num: the number of topics<br>
 * sample num: after gibbs sampling burn in period, we sample |sample num| times and calculate their average value.<br>
 * data: path of the data<br>
 * model name: model's name. output file will be named by |model_name|.|type|<br>
*Note that* alpha and beta are set as 50/K and 0.1 as default.<br>
*e.g.* $> ./lda 100 50 ap.data ap
= Top Words of Each Topic=
   $> ./topword |phi_file| |vocab_file||out_file|
 * phi_file: phi file generated by lda program
 * vocab_file: vocabulary list of the training corpus
 * out_file: output file name
= Input Data Formation=
Input data of this program is the same as blei's C code of LDA [http://www.cs.princeton.edu/~blei/lda-c/ Here]<br>
 * |M| |term_1|:|count| |term_2|:|count| ...  |term_N|:|count|<br>
 * |M| is the number of unique terms in the document<br>
 * |count| associated with each term is how many times that term appeared in the document. <br> 
*Note that* |term_1| is an integer which indexes the term; it is not a string.<br>
= Output Data = 
Output files contain:<br>
 * |model_name|.theta : estimation for document-topic parameter
 * |model_name|.phi : estimation for topic-word parameter
 * |model_name|.tassgin : topic assigned to each word
 * |model_name|.other : alpha,beta,topic sum,sample sum
= Case Study =
To apply LDA on trn.dat in model.zip:<br>
== Compile LDA ==
   $> make clean<br>
   $> make<br>
   $> make topword<br>
== Run LDA ==
   $> ./lda 100 50 trn.data trn
== Time Consuming, a Comparative Perspectives == 
 My computer:<br>
 openSUSE 11 <br>
 GCC 4.2<br>
 Intel Core Duo CPU T2450<br>
 2GB RAM<br>
 * ldaviagibbs program: 1000 iterations of burn in period cost 26 minutes and the following 50 sampling cost 14 minutes. memory consuming: 16.8MB<br>
 * Blei's C code: 48 iterations to converge, cost 1h22min, memory consuming: 20.5MB<br>
 * GibbsLDA++: 1000 iterations cost 23min. memory consuming: 17.8MB<br>
== Top Words of Each Topic ==
   $> topword trn.phi vocab.txt topword 
= Data Analysis =
Following lists are parts of the output of the topword program<br>
popular words in topic 1 such as federal, law, government, security etc. are related to policy, so we can name topic 1 as policy.<br>
popular words in topic 2 such as smoking,tobacco, cigarette,ban, health are related to the  issue of smoking, so we can name topic 2 as smoking<br>
popular words in topic 3 such as white house, national, secretary, spokesman are related to government, so we can name topic 3 as government<br>
|| topic 1 || topic 2 || topic 3 ||
||program	||report	||reagan||
||report	||commission	||house||
||federal	||public	||white||
||law	||smoking	||president||
||programs	||services	||administration||
||government	||ban	||fitzwater||
||security	||tobacco	||national||
||national	||cigarettes	||secretary||
||social	||product	||reagans||
||says	||health	||spokesman||
||public	||sand	||first||
||money	||reynolds	||office||
||system	||separate	||chief||
||take	||stations	||presidential||
||benefits	||safety	||marlin||
||private	||products	||security||
||agencies	||evidence	||staff||
||make	||consumer	||policy||
||agency	||rules	||made||
||act	||release	||presidents||
||changes	||premier	||aides||
||violations	||cigarette	||adviser||
||policy	||spokeswoman	||washington||
||get	||smoke	||public||
||enforcement	||taken	||word||